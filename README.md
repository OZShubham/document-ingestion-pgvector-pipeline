# ğŸŒŠ DIVE â€” Document Ingestion, Verification, and Embedding

> DIVE is an end-to-end document intelligence platform that ingests files, extracts and verifies content, produces embeddings, and exposes search and RAG chat capabilities.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![React](https://img.shields.io/badge/react-18+-61dafb.svg)](https://reactjs.org/)

This repository contains three main components:

- `frontend/` â€” React + Vite single-page app used by end users
- `backend/` â€” FastAPI service exposing REST and WebSocket endpoints
- `cloud_function/` â€” GCP Cloud Function (Gen2) pipeline that processes GCS uploads

Visit the component READMEs for more detail:

- `frontend/README.md` â€” frontend setup, build and deploy (Vite)
- `backend/README.md` â€” backend setup, env, run instructions (FastAPI)
- `cloud_function/README.md` â€” cloud function local testing and deploy

## Quicklinks
- Repo: https://github.com/OZShubham/document-ingestion-pgvector-pipeline
- Owner: Shubham Mishra

## TL;DR â€” What DIVE does

- Accepts document uploads (PDF, DOCX, XLSX, images, text, etc.)
- Extracts text and metadata using multiple processors (Gemini, PyMuPDF, PyPDF, LangChain processors) with smart fallbacks
- Chunks content with configurable chunking strategies and stores chunk previews
- Generates embeddings (Vertex AI / configured embedding provider) and stores vectors in Cloud SQL (Postgres + pgvector)
- Provides semantic search, analytics, and RAG-style chat with source citations
- Exposes real-time processing status via WebSockets

## Architecture (high level)

The repository implements a modular architecture. Important components:

- Frontend (React) â€” uploads files and shows processing status, search, and chat UI.
- Backend (FastAPI) â€” API and WebSocket server. Handles project management, document metadata, queries, and orchestrates search and chat.
- Cloud Storage (GCS) â€” stores uploaded files under `documents/{project_id}/...`.
- Cloud Function (Gen2) â€” triggered on new GCS objects; runs extraction, chunking, embedding, and stores results in the DB and vector table.
- Cloud SQL (Postgres) + pgvector â€” stores metadata, chunks, and vector embeddings.
- Embedding & LLM services â€” Vertex AI embeddings (or other provider configured via `config.py`) and Gemini or other LLMs for extraction and RAG responses. The pipeline uses many processors â€” Gemini is one of them, with local processors as fallbacks.

See `docs/ARCHITECTURE.md` (if present) or the folder READMEs for diagrams and deeper details.

## ğŸ“ Project Structure

```
dive/
â”œâ”€â”€ backend/                    # FastAPI Backend
â”‚   â”œâ”€â”€ main.py                # Main API application
â”‚   â”œâ”€â”€ config.py              # Configuration
â”‚   â”œâ”€â”€ database_manager.py   # Database connection
â”‚   â”œâ”€â”€ vector_store_manager.py # Vector operations
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile            # Container config
â”‚   â””â”€â”€ .env                  # Environment variables
â”‚
â”œâ”€â”€ cloud-function/            # Document Processing
â”‚   â”œâ”€â”€ main.py               # Cloud Function entry
â”‚   â”œâ”€â”€ pipeline_processor.py # Processing pipeline
â”‚   â”œâ”€â”€ gemini_processor.py   # Gemini integration
â”‚   â”œâ”€â”€ document_processors.py # Document handlers
â”‚   â”œâ”€â”€ chunking_strategies.py # Text chunking
â”‚   â”œâ”€â”€ config.py             # Configuration
â”‚   â”œâ”€â”€ requirements.txt      # Dependencies
â”‚   â””â”€â”€ .env.yaml            # Environment config
â”‚
â”œâ”€â”€ frontend/                  # React Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx           # Main application
â”‚   â”‚   â”œâ”€â”€ config.js         # API client
â”‚   â”‚   â”œâ”€â”€ components/       # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ProcessingDetail.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentExplorer.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ LiveStatusIndicator.jsx
â”‚   â”‚   â”‚   â””â”€â”€ MobileNav.jsx
â”‚   â”‚   â”œâ”€â”€ index.css         # Global styles
â”‚   â”‚   â””â”€â”€ main.jsx          # Entry point
â”‚   â”œâ”€â”€ package.json          # Dependencies
â”‚   â”œâ”€â”€ vite.config.js        # Vite configuration
â”‚   â”œâ”€â”€ tailwind.config.js    # Tailwind config
â”‚   â””â”€â”€ .env.local           # Environment variables
â”‚
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ API.md                # API documentation
â”‚   â”œâ”€â”€ DEPLOYMENT.md         # Deployment guide
â”‚   â””â”€â”€ ARCHITECTURE.md       # Architecture details
â”‚
â””â”€â”€ README.md                 # This file
```

---

## Endpoints (quick reference)

The backend exposes REST endpoints under `/api` and a WebSocket endpoint for real-time updates. This list is a concise reference; the running app serves full OpenAPI docs at `/docs`.

- Projects
  - GET  /api/projects?user_id={user_id}
  - POST /api/projects
  - DELETE /api/projects/{project_id}?user_id={user_id}

- Documents
  - GET  /api/documents?project_id={pid}&user_id={uid}
  - GET  /api/documents/{document_id}?project_id={pid}&user_id={uid}
  - POST /api/documents/filter  (advanced filters)
  - POST /api/documents/batch   (batch delete/tag/export)
  - DELETE /api/documents/{document_id}?project_id={pid}&user_id={uid}

- Uploads
  - POST /api/upload/signed-url  â€” request a signed PUT URL for direct GCS upload
  - (UI may use backend proxy endpoints to accept file uploads and write metadata)

- Search & Chat
  - POST /api/search  â€” semantic search over chunks
  - POST /api/chat    â€” RAG-style chat (project-scoped)
  - GET  /api/chat/history?project_id={pid}&user_id={uid}

- Analytics
  - GET /api/projects/{project_id}/analytics?user_id={uid}

- Members
  - GET  /api/projects/{project_id}/members?user_id={uid}
  - POST /api/projects/{project_id}/members
  - DELETE /api/projects/{project_id}/members/{member_user_id}?user_id={uid}

- WebSocket
  - WS /api/ws/{project_id}  â€” receive live processing updates for a project

Tip: start the backend locally and open `http://localhost:8000/docs` to view the live OpenAPI spec.

## Quickstart â€” run locally (Windows PowerShell)

Prerequisites: Python 3.10+, Node.js 18+, Docker (optional), gcloud CLI (for cloud deploy)

1) Frontend

```powershell
cd frontend
npm install
npm run dev
# Open http://localhost:5173
```

2) Backend

```powershell
cd backend
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
# (set up .env from .env.example)
$env:GOOGLE_APPLICATION_CREDENTIALS = (Resolve-Path .\secrets\credentials.json).Path
uvicorn main:app --reload --host 0.0.0.0 --port 8000
# Open http://localhost:8000/docs
```

3) Cloud Function (local test)

```powershell
cd cloud_function
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
functions-framework --target=process_document_upload --signature-type=cloudevent --port=8080
# POST a sample CloudEvent to http://localhost:8080/ to simulate a GCS finalized trigger
```

For full instructions and more configuration, see the component READMEs in each folder.

## Configuration & env

- Backend: `backend/.env.example` contains keys like `GCS_BUCKET_NAME`, `DB_*`, `GCP_PROJECT_ID`, `EMBEDDING_MODEL`, `GEMINI_MODEL`.
- Frontend: `frontend/.env.example` has `VITE_API_URL` and `VITE_GCS_BUCKET`.
- Cloud Function: `cloud_function/requirements.txt` and `cloud_function/main.py` drive the pipeline behavior.

Important note: service account credentials (file found at `backend/secrets/credentials.json` in this repo) are used for local dev only â€” in production use Workload Identity or Secret Manager.

## Database schema

The DB uses PostgreSQL with `pgvector` installed for vector storage. Core tables include `projects`, `members`, `documents`, `document_chunks`, `document_vectors`, and `processing_logs` (see `backend/migration.py` and `cloud_function/schema.sql` for schema and examples).

## How extraction and processing works (brief)

1. A user uploads a file (frontend or API). Backend stores metadata and either proxies the upload to GCS or returns a signed URL for direct upload.
2. When the object is finalized in GCS under `documents/{project_id}/...`, the Cloud Function is triggered.
3. The Cloud Function pipeline (`pipeline_processor.py`) chooses an extraction strategy:
   - Try Gemini-based extraction (if configured and available)
   - Fallback to local processors (PyMuPDF, pypdf, OCR, LangChain processors) depending on file type
4. Extracted text is chunked via configured chunking strategies (`chunking_strategies.py`) with configurable token/page size.
5. The pipeline generates embeddings (Vertex AI or configured embedding model), stores vectors in `document_vectors`, and updates `documents` and `processing_logs`.
6. The backend serves search/chat requests by querying vectors and optionally combining keyword filters (hybrid search) and then composing RAG prompts for the LLM.

This design intentionally mixes cloud LLM services (Gemini/Vertex) and on-host processors for robustness and cost control.

## Deployment notes

- The repo contains example `cloudbuild.yaml` files and `Dockerfile`s for the frontend and backend. For production deployments we recommend:
  - Backend: Cloud Run (managed) or Cloud Run for Anthos
  - Processing: Cloud Functions (Gen2) or Cloud Run jobs for heavy workloads
  - Database: Cloud SQL (Postgres) with pgvector or a managed vector DB

- Example deploy commands are present in the per-component READMEs. Remember to set IAM roles for the service account: Storage Admin (or Storage Object Admin), Cloud SQL Client, and Vertex AI User where applicable.

## Contributing

See `CONTRIBUTING.md` (if present). Basic workflow:

1. Fork the repo
2. Create a feature branch
3. Run tests and linters
4. Open a pull request

## License

This project is released under the MIT License â€” see `LICENSE`.

Built with â¤ï¸ â€” Shubham Mishra
https://github.com/OZShubham/document-ingestion-pgvector-pipeline
