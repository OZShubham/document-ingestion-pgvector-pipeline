# ğŸŒŠ DIVE â€” Document Ingestion, Verification, and Embedding

> DIVE is an end-to-end document intelligence platform that ingests files, extracts and verifies content, produces embeddings, and exposes search and RAG chat capabilities.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![React](https://img.shields.io/badge/react-18+-61dafb.svg)](https://reactjs.org/)

## ğŸš€ Project Demo

<p align="center">
  <a href="https://youtu.be/su4v_cKR8oM">
    <img src="https://img.youtube.com/vi/su4v_cKR8oM/maxresdefault.jpg" alt="Project DIVE Demo Video">
  </a>
</p>
<p align="center">
  <strong>Click the thumbnail above to watch the full project demo on YouTube!</strong>
</p>

## âœ¨ Key Features

DIVE is a comprehensive, production-ready platform. Here's what sets it apart:

### ğŸš€ **Frontend & User Experience**
* **Modern UI:** A sleek, responsive frontend built with **React**, **Vite**, and **Tailwind CSS**.
* **Project-Based Multi-tenancy:** Organize documents, teams, and analytics into isolated projects.
* **Team Collaboration:** Invite and manage team members with roles (owner, admin, member).
* **Real-time Processing:** A **WebSocket**-powered `LiveStatusIndicator` shows users the exact status of their documents as they move through the pipeline (e.g., "Processing," "Chunking").
* **Mobile-Ready:** Fully responsive design with a dedicated mobile navigation panel (`MobileNav.jsx`).

### ğŸ“ˆ **Analytics & Observability**
* **Full Analytics Dashboard:** A rich, real-time dashboard (`Dashboard.jsx`) showing:
    * Upload activity, success vs. failure rates.
    * Distribution of file types and processing methods.
    * Pipeline stage performance to identify bottlenecks.
    * Top contributors, vector store statistics, and recent errors.
* **"Glass Box" Processing View:** A detailed drill-down (`ProcessingDetail.jsx`) for *each* document showing:
    * A step-by-step **processing timeline** (from `_log_stage`).
    * A **chunk browser** to inspect the raw text chunks (from `getDocumentChunks`).
    * **AI-generated insights** (summary, key topics, keywords) from the AI Insights tab.
* **Advanced Document Explorer:** A "mission control" (`DocumentExplorer.jsx`) for all documents with:
    * Deep filtering (by status, uploader, file type, processing method, and more).
    * Multi-column sorting (by date, size, name, etc.).
    * **Batch operations** (e.g., select all and "Delete" or "Export").

### ğŸ§  **Intelligent Processing Pipeline**
* **Smart Gemini Integration:** Leverages **Gemini** for high-accuracy text, table, and image extraction from complex documents.
* **Resilient Fallbacks:** Intelligently routes files to the best tool. If a file is unsuitable for Gemini (e.g., >50MB), it automatically falls back to `PyMuPDF`, `pypdf`, `docx-python`, or `openpyxl`.
* **Large File Handling:**
    * **Size Limiting:** Pre-checks files to skip Gemini for files >50MB, saving cost and preventing errors.
    * **PDF Truncation:** Automatically truncates PDFs with >1000 pages to the first 1000, processes them, and adds a warning.
* **Duplicate & Update Handling:** The pipeline is idempotent:
    * **Skips Duplicates:** If an identical, completed file is re-uploaded, it's skipped.
    * **Handles Overwrites:** If a *new version* of a file is uploaded, DIVE automatically wipes the old vectors and chunks and re-processes the new file from scratch.
* **Multiple Chunking Strategies:** Includes `RecursiveCharacterTextSplitter`, `SemanticChunker`, and `SentenceSplitter` (NLTK).

### ğŸ’¬ **Search & RAG**
* **Semantic Search:** Fast, project-scoped vector search powered by **Vertex AI Embeddings** and **pgvector**.
* **RAG Chat Interface:** A "Chat with your documents" UI (`ChatInterface.jsx`) that generates AI-powered answers.
* **Verifiable Answers:** All chat answers include **source citations** with filename, page number, and similarity score.
* **Configurable Chat:** Users can adjust the AI's creativity (temperature) and the number of sources (k) to retrieve.

### ğŸ”§ **Architecture & Deployment**
* **Serverless & Scalable:** Built on a modern serverless stack: **GCP Cloud Run** (Backend), **GCP Cloud Function (Gen2)** (Pipeline), and **Cloud SQL** (Database).
* **Containerized:** Fully-containerized backend and frontend with `Dockerfile`s.
* **CI/CD Ready:** Includes `cloudbuild.yaml` files for automated deployment to GCP.
* **Event-Driven:** Uses GCS triggers for ingestion and Pub/Sub for notifications.

This repository contains three main components:

- `frontend/` â€” React + Vite single-page app used by end users
- `backend/` â€” FastAPI service exposing REST and WebSocket endpoints
- `cloud_function/` â€” GCP Cloud Function (Gen2) pipeline that processes GCS uploads

Visit the component READMEs for more detail:

- `frontend/README.md` â€” frontend setup, build and deploy (Vite)
- `backend/README.md` â€” backend setup, env, run instructions (FastAPI)
- `cloud_function/README.md` â€” cloud function local testing and deploy

## Quicklinks
- Repo: https://github.com/OZShubham/document-ingestion-pgvector-pipeline
- Owner: Shubham Mishra

## TL;DR â€” What DIVE does

- Accepts document uploads (PDF, DOCX, XLSX, images, text, etc.)
- Extracts text and metadata using multiple processors (Gemini, PyMuPDF, PyPDF, LangChain processors) with smart fallbacks
- Chunks content with configurable chunking strategies and stores chunk previews
- Generates embeddings (Vertex AI / configured embedding provider) and stores vectors in Cloud SQL (Postgres + pgvector)
- Provides semantic search, analytics, and RAG-style chat with source citations
- Exposes real-time processing status via WebSockets

## Architecture (high level)

The repository implements a modular architecture. Important components:

- Frontend (React) â€” uploads files and shows processing status, search, and chat UI.
- Backend (FastAPI) â€” API and WebSocket server. Handles project management, document metadata, queries, and orchestrates search and chat.
- Cloud Storage (GCS) â€” stores uploaded files under `documents/{project_id}/...`.
- Cloud Function (Gen2) â€” triggered on new GCS objects; runs extraction, chunking, embedding, and stores results in the DB and vector table.
- Cloud SQL (Postgres) + pgvector â€” stores metadata, chunks, and vector embeddings.
- Embedding & LLM services â€” Vertex AI embeddings (or other provider configured via `config.py`) and Gemini or other LLMs for extraction and RAG responses. The pipeline uses many processors â€” Gemini is one of them, with local processors as fallbacks.

See `docs/ARCHITECTURE.md` (if present) or the folder READMEs for diagrams and deeper details.

## ğŸ“ Project Structure

```
dive/
â”œâ”€â”€ backend/                    # FastAPI Backend
â”‚   â”œâ”€â”€ main.py                # Main API application
â”‚   â”œâ”€â”€ config.py              # Configuration
â”‚   â”œâ”€â”€ database_manager.py   # Database connection
â”‚   â”œâ”€â”€ vector_store_manager.py # Vector operations
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile            # Container config
â”‚   â””â”€â”€ .env                  # Environment variables
â”‚
â”œâ”€â”€ cloud-function/            # Document Processing
â”‚   â”œâ”€â”€ main.py               # Cloud Function entry
â”‚   â”œâ”€â”€ pipeline_processor.py # Processing pipeline
â”‚   â”œâ”€â”€ gemini_processor.py   # Gemini integration
â”‚   â”œâ”€â”€ document_processors.py # Document handlers
â”‚   â”œâ”€â”€ chunking_strategies.py # Text chunking
â”‚   â”œâ”€â”€ config.py             # Configuration
â”‚   â”œâ”€â”€ requirements.txt      # Dependencies
â”‚   â””â”€â”€ .env.yaml            # Environment config
â”‚
â”œâ”€â”€ frontend/                  # React Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx           # Main application
â”‚   â”‚   â”œâ”€â”€ config.js         # API client
â”‚   â”‚   â”œâ”€â”€ components/       # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ProcessingDetail.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentExplorer.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ LiveStatusIndicator.jsx
â”‚   â”‚   â”‚   â””â”€â”€ MobileNav.jsx
â”‚   â”‚   â”œâ”€â”€ index.css         # Global styles
â”‚   â”‚   â””â”€â”€ main.jsx          # Entry point
â”‚   â”œâ”€â”€ package.json          # Dependencies
â”‚   â”œâ”€â”€ vite.config.js        # Vite configuration
â”‚   â”œâ”€â”€ tailwind.config.js    # Tailwind config
â”‚   â””â”€â”€ .env.local           # Environment variables
â”‚
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ API.md                # API documentation
â”‚   â”œâ”€â”€ DEPLOYMENT.md         # Deployment guide
â”‚   â””â”€â”€ ARCHITECTURE.md       # Architecture details
â”‚
â””â”€â”€ README.md                 # This file
```

---

## Endpoints (quick reference)

The backend exposes REST endpoints under `/api` and a WebSocket endpoint for real-time updates. This list is a concise reference; the running app serves full OpenAPI docs at `/docs`.

- Projects
  - GET  /api/projects?user_id={user_id}
  - POST /api/projects
  - DELETE /api/projects/{project_id}?user_id={user_id}

- Documents
  - GET  /api/documents?project_id={pid}&user_id={uid}
  - GET  /api/documents/{document_id}?project_id={pid}&user_id={uid}
  - POST /api/documents/filter  (advanced filters)
  - POST /api/documents/batch   (batch delete/tag/export)
  - DELETE /api/documents/{document_id}?project_id={pid}&user_id={uid}

- Uploads
  - POST /api/upload/signed-url  â€” request a signed PUT URL for direct GCS upload
  - (UI may use backend proxy endpoints to accept file uploads and write metadata)

- Search & Chat
  - POST /api/search  â€” semantic search over chunks
  - POST /api/chat    â€” RAG-style chat (project-scoped)
  - GET  /api/chat/history?project_id={pid}&user_id={uid}

- Analytics
  - GET /api/projects/{project_id}/analytics?user_id={uid}

- Members
  - GET  /api/projects/{project_id}/members?user_id={uid}
  - POST /api/projects/{project_id}/members
  - DELETE /api/projects/{project_id}/members/{member_user_id}?user_id={uid}

- WebSocket
  - WS /api/ws/{project_id}  â€” receive live processing updates for a project

Tip: start the backend locally and open `http://localhost:8000/docs` to view the live OpenAPI spec.

## Quickstart â€” run locally (Windows PowerShell)

Prerequisites: Python 3.10+, Node.js 18+, Docker (optional), gcloud CLI (for cloud deploy)

1) Frontend

```powershell
cd frontend
npm install
npm run dev
# Open http://localhost:5173
```

2) Backend

```powershell
cd backend
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
# (set up .env from .env.example)
$env:GOOGLE_APPLICATION_CREDENTIALS = (Resolve-Path .\secrets\credentials.json).Path
uvicorn main:app --reload --host 0.0.0.0 --port 8000
# Open http://localhost:8000/docs
```

3) Cloud Function (local test)

```powershell
cd cloud_function
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
functions-framework --target=process_document_upload --signature-type=cloudevent --port=8080
# POST a sample CloudEvent to http://localhost:8080/ to simulate a GCS finalized trigger
```

For full instructions and more configuration, see the component READMEs in each folder.

## Configuration & env

- Backend: `backend/.env.example` contains keys like `GCS_BUCKET_NAME`, `DB_*`, `GCP_PROJECT_ID`, `EMBEDDING_MODEL`, `GEMINI_MODEL`.
- Frontend: `frontend/.env.example` has `VITE_API_URL` and `VITE_GCS_BUCKET`.
- Cloud Function: `cloud_function/requirements.txt` and `cloud_function/main.py` drive the pipeline behavior.

Important note: service account credentials (file found at `backend/secrets/credentials.json` in this repo) are used for local dev only â€” in production use Workload Identity or Secret Manager.

## Database schema

The DB uses PostgreSQL with `pgvector` installed for vector storage. Core tables include `projects`, `members`, `documents`, `document_chunks`, `document_vectors`, and `processing_logs` (see `backend/migration.py` and `cloud_function/schema.sql` for schema and examples).

## How extraction and processing works (brief)

1. A user uploads a file (frontend or API). Backend stores metadata and either proxies the upload to GCS or returns a signed URL for direct upload.
2. When the object is finalized in GCS under `documents/{project_id}/...`, the Cloud Function is triggered.
3. The Cloud Function pipeline (`pipeline_processor.py`) chooses an extraction strategy:
   - Try Gemini-based extraction (if configured and available)
   - Fallback to local processors (PyMuPDF, pypdf, OCR, LangChain processors) depending on file type
4. Extracted text is chunked via configured chunking strategies (`chunking_strategies.py`) with configurable token/page size.
5. The pipeline generates embeddings (Vertex AI or configured embedding model), stores vectors in `document_vectors`, and updates `documents` and `processing_logs`.
6. The backend serves search/chat requests by querying vectors and optionally combining keyword filters (hybrid search) and then composing RAG prompts for the LLM.

This design intentionally mixes cloud LLM services (Gemini/Vertex) and on-host processors for robustness and cost control.

## Deployment notes

- The repo contains example `cloudbuild.yaml` files and `Dockerfile`s for the frontend and backend. For production deployments we recommend:
  - Backend: Cloud Run (managed) or Cloud Run for Anthos
  - Processing: Cloud Functions (Gen2) or Cloud Run jobs for heavy workloads
  - Database: Cloud SQL (Postgres) with pgvector or a managed vector DB

- Example deploy commands are present in the per-component READMEs. Remember to set IAM roles for the service account: Storage Admin (or Storage Object Admin), Cloud SQL Client, and Vertex AI User where applicable.

## Contributing

See `CONTRIBUTING.md` (if present). Basic workflow:

1. Fork the repo
2. Create a feature branch
3. Run tests and linters
4. Open a pull request

## License

This project is released under the MIT License â€” see `LICENSE`.

Built with â¤ï¸ â€” Shubham Mishra
https://github.com/OZShubham/document-ingestion-pgvector-pipeline
